{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "444ed15f-f60c-40a1-bbd3-bfdf48752730",
   "metadata": {},
   "source": [
    "# Documentación de Funciones\n",
    "- En este Notebook únicamente añadiremos comentarios para algunas funciones.\n",
    "- El código funcional se encuentra en `src/ft_functions.py` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154de3d7-bca1-4c4e-90ac-e5f85216f88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63790d99-8722-4214-968d-d1faf07051c9",
   "metadata": {},
   "source": [
    "## Función Sofmax\n",
    "- Para un caso multinomial, en lugar de la función sigmoide, es recomendado usar la función softmax.  \n",
    "- La función sigmoide se usa principalmente para clasificación binaria, mientras que softmax es la generalización para múltiples clases.\n",
    "- La función softmax se define como:\n",
    "- softmax(z)_i = exp(z_i) / Σ(exp(z_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ecb82a9-58c5-4c3a-961a-006f9cfbdd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"\n",
    "    Calcula la función softmax para clasificación multinomial\n",
    "\n",
    "    Parámetros:\n",
    "    z: matriz de forma (n_muestras, n_clases)\n",
    "\n",
    "    Retorna:\n",
    "    matriz de probabilidades de forma (n_muestras, n_clases)\n",
    "    donde cada fila suma 1\n",
    "    \"\"\"\n",
    "    # Restamos el máximo para estabilidad numérica\n",
    "    # Esto evita desbordamiento en exp() con números grandes\n",
    "    z_shifted = z - np.max(z, axis=1, keepdims=True)\n",
    "\n",
    "    # Calculamos exp() de los valores desplazados\n",
    "    exp_scores = np.exp(z_shifted)\n",
    "\n",
    "    # Normalizamos dividiendo por la suma\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7a2717-f16e-4c77-b85f-eddc4aeb46ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de prueba:\n",
      " [[ 1  2  3  4]\n",
      " [ 2  1  0 -1]\n",
      " [ 0  0  0  0]]\n",
      "\n",
      "Probabilidades softmax:\n",
      " [[0.0320586  0.08714432 0.23688282 0.64391426]\n",
      " [0.64391426 0.23688282 0.08714432 0.0320586 ]\n",
      " [0.25       0.25       0.25       0.25      ]]\n",
      "\n",
      "Verificar que cada fila suma 1:\n",
      " [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# Podemos probar la función con algunos valores\n",
    "test_values = np.array([\n",
    "    [1, 2, 3, 4],\n",
    "    [2, 1, 0, -1],\n",
    "    [0, 0, 0, 0]\n",
    "])\n",
    "print(\"Valores de prueba:\\n\", test_values)\n",
    "print(\"\\nProbabilidades softmax:\\n\", softmax(test_values))\n",
    "print(\"\\nVerificar que cada fila suma 1:\\n\", np.sum(softmax(test_values), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c1a83-68a4-4253-b26d-b1166e13211a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "979b399b-d513-4c6d-a93b-7119aeed8d6a",
   "metadata": {},
   "source": [
    "## Función de pérdida\n",
    "- Para el caso multinomial, necesitamos adaptar la función de pérdida para manejar múltiples clases.\n",
    "- La función de pérdida logarítmica multinomial también se llama cross-entropy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29da420a-9b48-4d54-9465-8001c71ed034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, W):\n",
    "    \"\"\"\n",
    "    Calcula la función de pérdida logarítmica (cross-entropy) para clasificación multinomial\n",
    "\n",
    "    Parámetros:\n",
    "    X: matriz de características (incluyendo columna de 1's) de forma (n_muestras, n_características)\n",
    "    y: matriz one-hot de etiquetas reales de forma (n_muestras, n_clases)\n",
    "    W: matriz de pesos de forma (n_características, n_clases)\n",
    "\n",
    "    Retorna:\n",
    "    J: valor de la función de pérdida\n",
    "    \"\"\"\n",
    "    m = X.shape[0]  # número de muestras\n",
    "\n",
    "    # Calcular predicciones\n",
    "    z = np.dot(X, W)  # (n_muestras, n_clases)\n",
    "    h = softmax(z)    # (n_muestras, n_clases)\n",
    "\n",
    "    # Calcular pérdida logarítmica\n",
    "    epsilon = 1e-15  # para evitar log(0)\n",
    "\n",
    "    # Multiplicación elemento a elemento de y real con log de predicciones\n",
    "    # y sumamos sobre todas las clases (axis=1) y todas las muestras\n",
    "    J = -(1/m) * np.sum(y * np.log(h + epsilon))\n",
    "\n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54e0164c-ca18-41c2-b536-4e44a4ebebf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "# Supongamos que tenemos:\n",
    "# X: (1508, 8) - 1508 muestras, 7 características + 1 columna de unos\n",
    "# y: (1508, 4) - etiquetas one-hot para 4 casas\n",
    "# W: (8, 4) - pesos para cada característica y cada clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa77ef2-a65b-4c91-a531-ae1c498eb9fe",
   "metadata": {},
   "source": [
    "### Principales diferencias con la versión binaria\n",
    "- Usamos W (matriz de pesos) en lugar de theta (vector)\n",
    "- Usamos softmax en lugar de sigmoid\n",
    "- La fórmula de la pérdida es más simple porque y es one-hot encoding (solo el término positivo importa)\n",
    "- No necesitamos el término (1-y) porque las etiquetas ya están en formato one-hot\n",
    "\n",
    "Esta función de pérdida penalizará más cuando el modelo asigne probabilidades bajas a las clases correctas y nos servirá para entrenar el modelo mediante descenso por gradiente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534be40-22ee-4fb9-858e-49063b73799f",
   "metadata": {},
   "source": [
    "## Función predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e541b1e-3f4a-4993-b4d4-cda9d58037c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, W):\n",
    "    \"\"\"\n",
    "    Realiza predicciones usando los pesos aprendidos\n",
    "\n",
    "    Parámetros:\n",
    "    X: matriz de características (incluyendo columna de 1's)\n",
    "    W: matriz de pesos optimizada\n",
    "\n",
    "    Retorna:\n",
    "    predicciones: matriz de probabilidades para cada clase\n",
    "    \"\"\"\n",
    "    z = np.dot(X, W)\n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa5e119-39f0-47f4-864b-b1440573eeee",
   "metadata": {},
   "source": [
    "## Grabación de los pesos óptimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95e45857-f53a-4fd3-afd8-430f39b95035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weights(W, output_file='../output/model_weights.json'):\n",
    "    \"\"\"\n",
    "    Guarda los pesos del modelo en formato JSON\n",
    "    \n",
    "    Parámetros:\n",
    "    W: matriz de pesos numpy del modelo\n",
    "    output_dir: directorio donde guardar el archivo\n",
    "    \"\"\"    \n",
    "    # Convertir matriz de pesos numpy a lista\n",
    "    weights = W.tolist()\n",
    "    \n",
    "    # Guardar en JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(weights, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
