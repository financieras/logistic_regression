{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzRXAT4X2h46"
   },
   "source": [
    "# Logistic Regression\n",
    "Proyecto de regresión logística multinomial para clasificar estudiantes en las casas de Hogwarts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cs_wUWRWzVHC"
   },
   "source": [
    "## Estructura del proyecto\n",
    "\n",
    "### datasets/\n",
    "- [dataset_test.csv](../datasets/dataset_test.csv)\n",
    "- [dataset_train.csv](../datasets/dataset_train.csv)\n",
    "- normal_test.csv\n",
    "- normal_test_pre_imputation.csv\n",
    "- normal_train.csv\n",
    "\n",
    "\n",
    "\n",
    "### notebooks/\n",
    "0. main.ipynb\n",
    "1. [exploratory.ipynb](exploratory.ipynb)\n",
    "2. [describe.ipynb](describe.ipynb)\n",
    "3. [histogram.ipynb](histogram.ipynb)\n",
    "4. [scatter_plot.ipynb](scatter_plot.ipynb)\n",
    "5. [pair_plot.ipynb](pair_plot.ipynb)\n",
    "6. [normalize.ipynb](normalize.ipynb)\n",
    "7. [imputation.ipynb](imputation.ipynb)\n",
    "8. [logreg_train.ipynb](logreg_train.ipynb)\n",
    "9. [logreg_sgd_train.ipynb](logreg_sgd_train.ipynb)\n",
    "10. [logreg_minibatch_train.ipynb](logreg_minibatch_train.ipynb)\n",
    "11. [logreg_predict.ipynb](logreg_predict.ipynb)\n",
    "12. [functions_documentation.ipynb](functions_documentation.ipynb)\n",
    "\n",
    "### src/\n",
    "- [ft_functions.py](../src/ft_functions.py)\n",
    "\n",
    "### output/\n",
    "- colum_to_drop.json\n",
    "- correlation_heatmap.png\n",
    "- histogram.png\n",
    "- houses2.csv\n",
    "- houses3.csv\n",
    "- houses.csv\n",
    "- model_weights2.json\n",
    "- model_weights3.json\n",
    "- model_weights.json\n",
    "- pair_plot.png\n",
    "- perfect_correlation_plot.png\n",
    "- second_strongest_correlation_plot.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contenido de los Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. exploratory.ipynb\n",
    "1. Análisis exploratorio inicial:\n",
    "   * Carga de datasets de entrenamiento (1600 registros) y test (400 registros).\n",
    "   * Identificación de la variable objetivo 'Hogwarts House'.\n",
    "   * Comparación de estructuras entre ambos datasets.\n",
    "2. Exploración de calidad de datos:\n",
    "   * Análisis de valores nulos por columna.\n",
    "   * Distribución de variables categóricas.\n",
    "   * Identificación de desbalances en clases.\n",
    "3. Análisis de correlaciones:\n",
    "   * Generación de matriz de correlaciones.\n",
    "   * Visualización mediante mapa de calor.\n",
    "   * Guardado de visualización en `correlation_heatmap.png`.\n",
    "4. Identificación de redundancias:\n",
    "   * Detección de correlación perfecta entre 'Astronomy' y 'Defense Against the Dark Arts'.\n",
    "   * Análisis de impacto de eliminación de variables.\n",
    "   * Guardado de decisión en `column_to_drop.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. describe.ipynb\n",
    "1. Análisis estadístico descriptivo:\n",
    "   * Verificación de directorio de trabajo.\n",
    "   * Importación de funciones estadísticas personalizadas.\n",
    "2. Implementación de funciones clave:\n",
    "   * `calculate_metrics()`: Cálculo de estadísticas por columna.\n",
    "   * `print_metrics_table()`: Visualización formateada.\n",
    "   * `analyze_dataset()`: Coordinación del análisis.\n",
    "3. Cálculo de métricas estadísticas:\n",
    "   * Medidas de tendencia central (media, mediana).\n",
    "   * Medidas de dispersión (desviación estándar, IQR).\n",
    "   * Medidas de forma (asimetría, curtosis).\n",
    "   * Coeficiente de variación (CV).\n",
    "4. Presentación de resultados:\n",
    "   * Tabla formateada usando `tabulate`.\n",
    "   * Una fila por métrica, una columna por asignatura."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. histogram.ipynb\n",
    "1. Visualización de distribuciones de puntuaciones:\n",
    "   * Lectura del dataset de entrenamiento.\n",
    "   * Identificación de columnas de cursos.\n",
    "   * Exclusión de 'Astronomy'.\n",
    "2. Generación de histogramas:\n",
    "   * Creación de matriz 4x3 de subgráficos.\n",
    "   * Un histograma por curso.\n",
    "   * Diferenciación por casas usando colores.\n",
    "   * Superposición de distribuciones para comparación.\n",
    "3. Análisis de homogeneidad:\n",
    "   * Identificación de cursos con distribución homogénea.\n",
    "   * Evaluación detallada de tres candidatos:\n",
    "     - Arithmancy\n",
    "     - Potions\n",
    "     - Care of Magical Creatures\n",
    "4. Conclusiones:\n",
    "   * 'Care of Magical Creatures' identificado como el curso más homogéneo.\n",
    "5. Generación de visualización:\n",
    "   * Guardado del histograma completo en `output/histogram.png`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. scatter_plot.ipynb\n",
    "1. Análisis de correlaciones entre asignaturas:\n",
    "   * Lectura del dataset de entrenamiento.\n",
    "   * Limpieza de datos eliminando filas incompletas.\n",
    "   * Selección de asignaturas a analizar.\n",
    "2. Visualización de la correlación perfecta:\n",
    "   * Scatter plot entre 'Astronomy' y 'Defense Against the Dark Arts'.\n",
    "   * Línea de tendencia.\n",
    "   * Coeficiente de correlación (r=-1.000).\n",
    "3. Búsqueda de correlaciones fuertes:\n",
    "   * Cálculo de matriz de correlación.\n",
    "   * Identificación de la segunda correlación más fuerte.\n",
    "4. Visualización de resultados:\n",
    "   * Scatter plot de la segunda correlación más fuerte.\n",
    "   * Generación de gráficos en la carpeta `output/`.\n",
    "   * Líneas de tendencia y coeficientes de correlación.\n",
    "5. Identificación de características similares:\n",
    "   * Primera pareja: 'Astronomy' y 'Defense Against the Dark Arts'.\n",
    "   * Segunda pareja: 'History of Magic' y 'Flying'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. pair_plot.ipynb\n",
    "1. Visualización de relaciones entre asignaturas:\n",
    "   * Lectura del dataset de entrenamiento.\n",
    "   * Eliminación de filas incompletas.\n",
    "   * Exclusión de 'Astronomy' (límite de 12 características).\n",
    "2. Generación de pair plot:\n",
    "   * Histogramas de distribución en la diagonal.\n",
    "   * Scatter plots entre pares de variables.\n",
    "   * Diferenciación por casas usando colores.\n",
    "3. Análisis de características:\n",
    "   * Identificación de las 5 mejores asignaturas para la clasificación:\n",
    "     - 'Defense Against the Dark Arts'\n",
    "     - 'Herbology'\n",
    "     - 'Potions'\n",
    "     - 'Charms'\n",
    "     - 'Flying'\n",
    "   * Identificación de asignaturas menos útiles por su alta superposición.\n",
    "4. Generación de visualización:\n",
    "   * Guardado del pair plot en `output/pair_plot.png`.\n",
    "5. Selección final de características para el modelo de regresión logística."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. normalize.ipynb\n",
    "1. Limpieza y normalización inicial de los dos DataFrames.\n",
    "2. Imputación de datos faltantes en 'Defense Against the Dark Arts' usando la correlación perfecta con 'Astronomy', en train y test.\n",
    "3. Eliminación de columnas innecesarias, incluyendo 'Astronomy'.\n",
    "4. Cálculo de la edad a partir de fechas.\n",
    "5. Conversión de datos categóricos a numéricos ('Best Hand' a float).\n",
    "6. One-hot encoding de 'Hogwarts House' para el dataset de entrenamiento.\n",
    "7. Normalización de variables numéricas.\n",
    "    - Normalización con media y desviación estándar de los datos de entrenamiento.\n",
    "    - Se procesa el conjunto de test de manera idéntica, usando los mismos parámetros de normalización.\n",
    "8. Guardado de datasets: normal_train.csv (completo) y normal_test_pre_imputation.csv (con valores faltantes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. imputation.ipynb\n",
    "1. Carga del dataset de test normalizado (normal_test_pre_imputation.csv).\n",
    "2. Implementación del algoritmo de k-vecinos más próximos (KNN) para imputación:\n",
    "    - Cálculo de distancias euclidianas.\n",
    "    - Identificación de k vecinos más cercanos.\n",
    "    - Imputación basada en la media de los vecinos.\n",
    "3. Aplicación de la imputación KNN a los registros incompletos.\n",
    "4. erificación y validación de la imputación.\n",
    "5. Guardado del dataset final completo (normal_test.csv)..ipynb:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. logreg_train.ipynb\n",
    "1. Entrenamiento del modelo de regresión logística multinomial.\n",
    "2. Implementación de funciones clave:\n",
    "    - Softmax para clasificación multinomial.\n",
    "    - Función de pérdida logarítmica (cross-entropy).\n",
    "    - Descenso del gradiente adaptado para el caso multinomial.\n",
    "    - Función de predicción.\n",
    "3. Proceso de entrenamiento:\n",
    "    - Inicialización de pesos aleatorios.\n",
    "    - Optimización mediante descenso del gradiente.\n",
    "    - Validación con datos de entrenamiento.\n",
    "4. Visualización de resultados:\n",
    "    - Gráfica de evolución del coste.\n",
    "    - Mapa de calor de importancia de características por casa.\n",
    "5. Guardado de los pesos óptimos del modelo en formato JSON.\n",
    "    - Los pesos se almacenan en `output/model_weights.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. logreg_sgd_train.ipynb\n",
    "1. Implementación de Descenso del Gradiente Estocástico (SGD):\n",
    "   * Procesamiento de una muestra a la vez.\n",
    "   * Actualización de pesos después de cada muestra.\n",
    "   * Aleatorización del orden de las muestras.\n",
    "2. Estructura del entrenamiento:\n",
    "   * Carga de datos normalizados.\n",
    "   * Preparación de matrices X e y.\n",
    "   * Implementación de funciones softmax y cálculo de coste.\n",
    "3. Proceso de entrenamiento SGD:\n",
    "   * Entrenamiento por épocas.\n",
    "   * Learning rate de 0.1.\n",
    "   * 200 épocas de entrenamiento.\n",
    "   * Early stopping con epsilon.\n",
    "4. Visualización y resultados:\n",
    "   * Gráfica de evolución del coste.\n",
    "   * Mapa de calor de importancia de características.\n",
    "   * Cálculo de precisión del modelo.\n",
    "5. Guardado del modelo:\n",
    "   * Almacenamiento de pesos en `model_weights.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. logreg_minibatch_train.ipynb\n",
    "1. Implementación de Descenso del Gradiente Mini-Batch:\n",
    "   * Procesamiento de subconjuntos de datos (batch_size=32).\n",
    "   * Balance entre GD por lotes y SGD.\n",
    "   * Actualización de pesos después de cada mini-batch.\n",
    "2. Estructura del entrenamiento:\n",
    "   * Carga de datos normalizados.\n",
    "   * Preparación de matrices X e y.\n",
    "   * Implementación de funciones softmax y cálculo de coste.\n",
    "3. Proceso de entrenamiento Mini-Batch:\n",
    "   * Entrenamiento por épocas con aleatorización.\n",
    "   * Learning rate de 0.1.\n",
    "   * 100 épocas de entrenamiento.\n",
    "   * Early stopping con epsilon.\n",
    "4. Visualización y resultados:\n",
    "   * Gráfica de evolución del coste.\n",
    "   * Mapa de calor de importancia de características.\n",
    "   * Cálculo de precisión del modelo.\n",
    "5. Guardado del modelo:\n",
    "   * Almacenamiento de pesos en `model_weights.json`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. logreg_predict.md\n",
    "1. Carga y preparación de datos:\n",
    "   * Lectura del dataset de test normalizado.\n",
    "   * Preparación de los datos de test con la función `prepare_test_data`.\n",
    "2. Gestión de múltiples modelos:\n",
    "   * Definición de pares modelo-output para tres versiones diferentes.\n",
    "   * Procesamiento de cada par `model_weights{n}.json` y `houses{n}.csv`.\n",
    "3. Predicción de casas:\n",
    "   * Carga de pesos óptimos para cada modelo.\n",
    "   * Cálculo de probabilidades y predicciones usando `make_house_predictions`.\n",
    "   * Generación de predicciones para los tres modelos en paralelo.\n",
    "4. Análisis comparativo:\n",
    "   * Creación de una tabla comparativa para los primeros 5 estudiantes.\n",
    "   * Visualización de probabilidades detalladas para cada casa y modelo.\n",
    "   * Comparación de predicciones entre los diferentes modelos.\n",
    "5. Generación y verificación de resultados:\n",
    "   * Almacenamiento de predicciones en múltiples archivos de salida.\n",
    "   * Verificación del formato del archivo `houses.csv`.\n",
    "   * Comparación de las predicciones entre los diferentes modelos generados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. functions_documentation.md\n",
    "1. Función Softmax:\n",
    "   * Implementación de softmax para clasificación multinomial.\n",
    "   * Explicación de la estabilidad numérica mediante desplazamiento.\n",
    "   * Demostración con casos de prueba.\n",
    "2. Función de Pérdida (Cross-entropy):\n",
    "   * Adaptación para clasificación multinomial.\n",
    "   * Cálculo de pérdida logarítmica con protección contra log(0).\n",
    "   * Comparación con la versión binaria.\n",
    "3. Función de Predicción:\n",
    "   * Implementación simple usando softmax.\n",
    "   * Generación de probabilidades para cada clase.\n",
    "4. Gestión de Pesos:\n",
    "   * Función para guardar pesos en formato JSON.\n",
    "   * Conversión de matrices numpy a formato serializable.\n",
    "5. Documentación General:\n",
    "   * Explicaciones detalladas de cada función.\n",
    "   * Ejemplos de uso y consideraciones prácticas.\n",
    "   * Referencias al código funcional en `src/ft_functions.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento con las tres variantes de Descenso del Gradiente\n",
    "- Tenemos implementadas las tres variantes del algoritmo de descenso de gradiente:\n",
    "1. ✅ GD (`logreg_train.ipynb`)\n",
    "2. ✅ SGD (`logreg_sgd_train.ipynb`)\n",
    "3. ✅ Mini-Batch GD (`logreg_minibatch_train.ipynb`)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNfv+4e9JE0HDXccPtrCcbu",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
